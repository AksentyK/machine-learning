{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52248863",
   "metadata": {},
   "source": [
    "# Working with S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a313ba00",
   "metadata": {},
   "source": [
    "One of the most common operations when working with [Amazon S3 (Amazon Simple Storage Service)](https://aws.amazon.com/s3/) is to pull data from s3 to local as well as push data from local to s3. We can use aws command line tool to achieve this:\n",
    "\n",
    "```bash\n",
    "# e.g. from s3 to local, add --recursive if it's a directory\n",
    "aws s3 cp <s3 path> <local path> --recursive\n",
    "```\n",
    "\n",
    "We'll also demonstrate how to use `boto3` to perform these kind of operations in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839c141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Ethen\n",
      "\n",
      "Last updated: 2023-04-04\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.8.10\n",
      "IPython version      : 8.4.0\n",
      "\n",
      "json   : 2.0.9\n",
      "numpy  : 1.23.2\n",
      "pyarrow: 5.0.0\n",
      "boto3  : 1.24.58\n",
      "pandas : 1.4.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from time import perf_counter\n",
    "from typing import List\n",
    "\n",
    "%watermark -a 'Ethen' -d -u -v -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c409771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace these top level configuration, especially s3 region and bucket\n",
    "region_name = \"\"\n",
    "bucket = \"\"\n",
    "s3_json_path = \"ethenliu/test.json\"\n",
    "s3_dir = \"ethenliu/data\"\n",
    "local_dir = \"ethenliu/data\"\n",
    "s3_parquet_path = os.path.join(s3_dir, \"test.parquet\")\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de9f49",
   "metadata": {},
   "source": [
    "Suppose we have a python object in memory, one option is to use client's [`put_object`](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/put_object.html) method and save it as a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dd1762e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [1, 2], 'embeddings': [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json dumps doesn't allow saving numpy array directly, we need to convert it to a list\n",
    "prediction = {\n",
    "    \"ids\": [1, 2],\n",
    "    \"embeddings\": np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]).tolist()\n",
    "}\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b855bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.put_object(\n",
    "    Body=json.dumps(prediction),\n",
    "    Bucket=bucket,\n",
    "    Key=s3_json_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b73cdb",
   "metadata": {},
   "source": [
    "All of this is well and good until we work with some large python objects, which we'll encounter [errors](https://stackoverflow.com/questions/26319815/entitytoolarge-error-when-uploading-a-5g-file-to-amazon-s3) such as entity too large error.\n",
    "\n",
    "Directly copied from S3's [documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/upload-objects.html)\n",
    "\n",
    "> - Upload an object in a single operation by using the AWS SDKs, REST API, or AWS CLI – With a single PUT operation, you can upload a single object up to 5 GB in size.\n",
    "> - Upload an object in parts by using the AWS SDKs, REST API, or AWS CLI – Using the multipart upload API operation, you can upload a single large object, up to 5 TB in size.\n",
    "\n",
    "Fortunately, we can rely on [`upload_file`](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-uploading-files.html) method, boto3 will automatically use [multipart upload underneath the hood](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html) without us having to worry about [lower level functions related to multipart upload](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html). The following code chunk shows how to save our python object as a parquet file and upload it to s3 as well as downloading files from s3 to local and reading it as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccce0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_parquet_to_s3(data, s3_bucket: str, s3_path: str, verbose: bool = False):\n",
    "    \"\"\"Saves the dictionary as a parquet file and push it to s3.    \n",
    "    \"\"\"\n",
    "    file_name = os.path.split(s3_path)[-1]\n",
    "    pa_table = pa.table(data)\n",
    "    pq.write_table(pa_table, file_name)\n",
    "\n",
    "    s3_client.upload_file(Filename=file_name, Bucket=s3_bucket, Key=s3_path)\n",
    "    os.remove(file_name)\n",
    "    if verbose:\n",
    "        print(pa_table)\n",
    "        print(\"Finish writing {} to s3://{}/{}\".format(file_name, s3_bucket, s3_path))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "166dec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 2d numpy array to list of 1d numpy array as pyarrow supports saving 1d numpy array\n",
    "embeddings = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "prediction = {\n",
    "    \"ids\": [1, 2],\n",
    "    \"embeddings\": [embedding for embedding in embeddings]\n",
    "}\n",
    "save_as_parquet_to_s3(prediction, bucket, s3_parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fd1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files_from_s3(\n",
    "    s3_bucket: str,\n",
    "    s3_dir: str,\n",
    "    local_dir: str\n",
    ") -> List[str]:\n",
    "    \"\"\"Download all files from a s3 path to local\"\"\"\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(s3_bucket)\n",
    "\n",
    "    objects = bucket.objects.filter(Prefix=s3_dir)\n",
    "    os.makedirs(local_dir, exist_ok=True)\n",
    "    # remove success file that are automatically saved by spark jobs\n",
    "    objects_key = [obj.key for obj in objects if \"_SUCCESS\" not in obj.key]\n",
    "\n",
    "    download_paths = []\n",
    "    for object_key in objects_key:\n",
    "        download_path = os.path.join(local_dir, os.path.split(object_key)[-1])\n",
    "        bucket.download_file(object_key, download_path)\n",
    "        download_paths.append(download_path)\n",
    "\n",
    "    return download_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bd8dbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0, 6.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids       embeddings\n",
       "0    1  [1.0, 2.0, 3.0]\n",
       "1    2  [4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists(local_dir):\n",
    "    files = [\n",
    "        os.path.join(local_dir, file_name)\n",
    "        for file_name in os.listdir(local_dir)\n",
    "    ]\n",
    "else:\n",
    "    print(\"download files from s3\")\n",
    "    start = perf_counter()\n",
    "    files = download_files_from_s3(bucket, s3_dir, local_dir)\n",
    "    end = perf_counter()\n",
    "    print(\"download files from s3 elapsed: \", end - start)\n",
    "\n",
    "df_list = [pd.read_parquet(file_name) for file_name in files]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0963406",
   "metadata": {},
   "source": [
    "Instead of downloading our parquet files to disk first, we can also read it directly into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "955d5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_object(s3_bucket: str, s3_dir: str):\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    bucket = s3.Bucket(s3_bucket)\n",
    "\n",
    "    objects = bucket.objects.filter(Prefix=s3_dir)\n",
    "    objects = [obj for obj in objects if \"_SUCCESS\" not in obj.key]\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eb984fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_s3_parquet_object(s3_object):\n",
    "    body = s3_object.get()[\"Body\"].read()\n",
    "    reader = pa.BufferReader(body)\n",
    "    table = pq.read_table(reader)\n",
    "    df = table.to_pandas()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a326b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[1.0, 2.0, 3.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[4.0, 5.0, 6.0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ids       embeddings\n",
       "0    1  [1.0, 2.0, 3.0]\n",
       "1    2  [4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_objects = list_s3_object(bucket, s3_dir)\n",
    "df_list = [read_s3_parquet_object(s3_object) for s3_object in s3_objects]\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef361d62",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb32e410",
   "metadata": {},
   "source": [
    "- [[1]](https://www.learnaws.org/2022/07/13/boto3-upload-files-s3/) How to use Boto3 to upload files to an S3 Bucket?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
